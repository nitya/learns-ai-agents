
# 1 | Fundamentals

Here's a quick rundown of 10 things you need to know about Azure AI Agent service before you dive into the Quickstart.

## 1.1 Definitions

The Azure AI Agent Service is **a fully managed service designed to empower developers to securely build, deploy, and scale high-quality, and extensible AI agents** without needing to manage the underlying compute and storage resources. 

An AI Agent acts as **a "smart" microservice** that can be used to answer questions (RAG), perform actions, or completely automate workflows - by **combining the power of generative AI models with tools that allow it to access and interact with real-world data sources.**

## 1.2 Create An Agent

The Azure AI Agent service uses the same wire protocol as Azure OpenAI Assistants allowing you to use either OpenAI SDKs or Azure AI Foundry SDK for building agents. To create an agent, you need to specify:

- the model - it uses to execute tasks
- the instructions - on how it should complete tasks
- the tools - it can acccess for support tasks
- the resources - it can use to support tools

**Here is a simple agent constructor**:

```python title="1 | Create An Agent"
# Create am agent with a model, instructions & tools
agent = project_client.agents.create_agent(
    model="gpt-4o-mini",
    name="my-agent",
    instructions="You are helpful agent",
    tools=code_interpreter.definitions,
    tool_resources=code_interpreter.resources,
)
```

## 1.3 Create An Activity Thread

To "run" an agent, we need an execution thread that is initialized with a message for that agent, typically consisting of a user request. Here's an example of what that looks like:

```python title="1 | Create Activity Thread"
# Create a thread with messages
thread = project_client.agents.create_thread()
message = project_client.agents.create_message(
    thread_id=thread.id,
    role="user",
    content="Could you please create a bar chart for the operating profit using the following data and provide the file to me? Company A: $1.2 million, Company B: $2.5 million, Company C: $3.0 million, Company D: $1.8 million",
)
```

## 1.4 Run Agent in Thread

To execute an agent, simply start a run with the created thread and agent as shown below. This runs the model (with instructions), invokes tools (as agent calls them) and returns final response (for user dispay). The messages now represent the _conversation_ between user and one or more agents, in the completion of the given task.

```python title="1 | Run Agent In Thread"
# Ask the agent to perform work on the thread
run = project_client.agents.create_and_process_run(thread_id=thread.id, agent_id=agent.id)

# Fetch and log all messages to see the agent's response
messages = project_client.agents.list_messages(thread_id=thread.id)
print(f"Messages: {messages}")
```
## 1.5 Coordinate Many Agents

To build more complex workflows, you can have agents sending instructions to other agents (effectively representing the user in the context of delegating that task) and using frameworks like Semantic Kernel or AutoGen to _orchestrate_ workflows and _coordinate_ conversations in effective ways, to deliver the end response.

Because Azure AI Agent Service works seamlessly with these frameworks **and is a managed service**, you can get all the benefits of security, scale, and safety, without added effort.

## 1.6 Benefits

Why use Azure AI Agent Service for orchestration instead of manually doing coordination using the [Inference API](https://learn.microsoft.com/en-us/azure/ai-studio/reference/reference-model-inference-api) for each model?

1. **Automatic tool calling** - now handled for you server-side, reducing client complexity
1. **Secure data management** - threads store conversation state for you
1. **Rich tool ecosystem** - includes file retrieval, code interpreter, Bing, Azure AI Search, Azure Functions

Why use Azure AI Agent Service for development instead of Azure OpenAI assistants?

1. **Flexible model selection** - pick models beyond Azure OpenAI e.g., Cohere, Llama 3
1. **Extensive data connectors** - integrate Bing, SharePoint, Fabric, AI Search, other API
1. **Enterprise grade security** - data privacy, compliance, keyless auth, secure data etc.
1. **Flexible storage options** - bring your own blog storage or use managed storage options

Plus, Azure AI Agent Service follows Responsible AI guidelines:

1. Responsible AI principles - for trustworthy AI
1. Content filters - for safety
1. Information & guidance - for customers

## 1.7 Limitations

1. Agent implementations have to work within the quota availability of underlying models
1. Agent AI Service has [other limits](https://learn.microsoft.com/en-us/azure/ai-services/agents/quotas-limits) for usage:
    - max files per agent/thread = 10K (API or Azure AI Foundry) or 20 (AOAI Studio)
    - max file size for agents = 512MB
    - max size for uploaded files = 100GB
    - agents token limit = 2M tokens
1. Agent AI Service supports [additional models](https://learn.microsoft.com/en-us/azure/ai-services/agents/concepts/model-region-support?tabs=python#additional-models). Just deploy them in the same project before using in agents. Models  currently limited to
    - Llama 3.1-70B-instruct
    - Mistral-large-2407
    - Cohere command R+


## 1.8 Cost

1. Understand [Azure OpenAI deployment types](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/deployment-types) to determine the differences in billing and performance for usage at scale.
1. Understand [Azure AI Foundry pricing](https://azure.microsoft.com/en-us/pricing/details/ai-foundry/) with respect to other AI services and tooling.

The [FAQ](https://learn.microsoft.com/en-us/azure/ai-services/agents/faq#how-am-i-charged-for-ai-agent-service-) breaks down other elements of cost including inference costs, tool usage costs, and storage costs.

## 1.9 Observability

Multi-agent workflows can be harder to debug since control flows through different threads and with different levels of parallelism and nesting.

[Tracing](https://learn.microsoft.com/en-us/azure/ai-services/agents/concepts/tracing) support gives you built-in observability using OpenTelemetry standards for data. Trace data can then be exported to relevant OpenTelemetryProtocol (OLTP) compatible dashboards - including by:

- using console tracing (locally)
- using Azure monitor (in Azure AI Foundry portal)


## 1.10 Available Tools

When you create AI agents, you can specify different tools in order to help ground the model they use, or extend their capabilities. These are categorized as:

1. **Knowledge Tools** - give the agent access to data sources for grounding responses. Think of use cases for agentic RAG.
1. **Action Tools** - enhance the agent's capabilities by allowing it to run various tools at runtime. Think of search, code interpreter etc.

---
